{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c234b74a-f36c-4cc8-8478-ed5f9777127d",
   "metadata": {},
   "source": [
    "# YOLOv8 Image Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf66c34-f91c-47d2-ba79-1fb49557b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-tensorrt sng4onnx onnx_graphsurgeon onnx onnxsim onnxruntime-gpu # tensorflow tflite_support onnx2tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11b0988-67bb-4fe5-814b-61b9e0718aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908d8be-620e-4be3-8463-abb2c2648a09",
   "metadata": {},
   "source": [
    "| Model | size (pixels) | acc top1 | acc top5 | Speed CPU ONNX (ms) | Speed A100 TensorRT (ms) | params (M) | FLOPs (B) at 640 |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| YOLOv8n-cls | 224 | 66.6 | 87.0 | 12.9 | 0.31 | 2.7 | 4.3 |\n",
    "| YOLOv8s-cls | 224 | 72.3 | 91.1 | 23.4 | 0.35 | 6.4 | 13.5 |\n",
    "| YOLOv8m-cls | 224 | 76.4 | 93.2 | 85.4 | 0.62 | 17.0 | 42.7 |\n",
    "| YOLOv8l-cls | 224 | 78.0 | 94.1 | 163.0 | 0.87 | 37.5 | 99.7 |\n",
    "| YOLOv8x-cls | 224 | 78.4 | 94.3 | 232.0 | 1.01 | 57.4 | 154.8 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff1b7d4-02d3-4149-b5b4-5cada2d17b74",
   "metadata": {},
   "source": [
    "## YOLOv8n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae1acf9-573e-470f-9fbe-5ba5bdd972ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n-cls.pt')  # load a pretrained model (recommended for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dbda060-1d64-454d-ac9f-3e6a87260bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.170 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=./data/Flower_Dataset, epochs=20, patience=50, batch=16, imgsz=64, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=48\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    391728  ultralytics.nn.modules.head.Classify         [256, 48]                     \n",
      "YOLOv8n-cls summary: 99 layers, 1499776 parameters, 1499776 gradients\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /opt/app/data/Flower_Dataset/train... 9206 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9206/9206 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       1/20     0.143G      1.009         16         64:   7%|â–‹         | 39/576 [00:02<00:20, 26.23it/s]Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
      "       1/20     0.143G     0.9918         16         64:  18%|â–ˆâ–Š        | 101/576 [00:04<00:15, 30.75it/s]\n",
      "       1/20     0.143G     0.9904         16         64:  18%|â–ˆâ–Š        | 101/576 [00:04<00:15, 30.75it/s]                  | 0.00/755k [00:00<?, ?B/s]\u001b[A\n",
      "       1/20     0.143G     0.9903         16         64:  18%|â–ˆâ–Š        | 105/576 [00:04<00:16, 28.77it/s]           | 104k/755k [00:00<00:00, 945kB/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 3.11MB/s]\u001b[A\n",
      "       1/20     0.143G      0.738          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:18<00:00, 31.89it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 28.08it/s]\n",
      "                   all      0.554      0.848\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       2/20     0.145G     0.3224          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:13<00:00, 43.95it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 49.33it/s] \n",
      "                   all      0.699      0.932\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       3/20     0.145G      0.202          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.60it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.07it/s] \n",
      "                   all      0.717       0.94\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       4/20     0.145G     0.1362          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 47.50it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.32it/s]\n",
      "                   all       0.75      0.946\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       5/20     0.145G    0.08342          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 47.35it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 50.58it/s] \n",
      "                   all      0.765      0.944\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       6/20     0.145G    0.04876          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 47.17it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 49.44it/s] \n",
      "                   all       0.77      0.949\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       7/20     0.145G    0.03421          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:15<00:00, 37.75it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 29.81it/s]\n",
      "                   all      0.768      0.944\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       8/20     0.145G     0.0222          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 47.46it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 57.69it/s] \n",
      "                   all      0.766      0.948\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       9/20     0.145G    0.01787          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.71it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 53.06it/s] \n",
      "                   all       0.77      0.945\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      10/20     0.145G    0.01281          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:11<00:00, 49.19it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 54.99it/s]\n",
      "                   all      0.783      0.952\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      11/20     0.145G    0.01083          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:11<00:00, 48.98it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 51.95it/s] \n",
      "                   all      0.777      0.954\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      12/20     0.145G   0.007744          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.71it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 55.20it/s]\n",
      "                   all      0.782      0.951\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      13/20     0.145G   0.006846          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:14<00:00, 40.87it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.25it/s] \n",
      "                   all      0.786       0.95\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      14/20     0.145G   0.006322          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.41it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:03<00:00, 27.31it/s]\n",
      "                   all      0.783      0.954\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      15/20     0.145G   0.003908          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:11<00:00, 49.95it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 54.64it/s] \n",
      "                   all      0.783      0.951\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      16/20     0.145G   0.003426          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:10<00:00, 53.28it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 54.18it/s] \n",
      "                   all      0.784      0.951\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      17/20     0.145G   0.002715          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 47.91it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.27it/s] \n",
      "                   all      0.783      0.952\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      18/20     0.145G   0.002655          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:11<00:00, 49.39it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 58.77it/s] \n",
      "                   all      0.789      0.952\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      19/20     0.145G   0.002336         16         64:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 285/576 [00:05<00:05, 52.52it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "      20/20     0.145G   0.001802          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:11<00:00, 48.86it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 53.26it/s] \n",
      "                   all      0.791      0.954\n",
      "\n",
      "20 epochs completed in 0.084 hours.\n",
      "Optimizer stripped from runs/classify/train/weights/last.pt, 3.1MB\n",
      "Optimizer stripped from runs/classify/train/weights/best.pt, 3.1MB\n",
      "Results saved to \u001b[1mruns/classify/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "results = model.train(data='./data/Flower_Dataset', epochs=20, imgsz=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab2e11-380b-4ad9-89ef-8d3b3d490d10",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82feba-be3b-4e9f-bd3c-d22fb28c760c",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_n_curves.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5331f7-5564-4e9e-ba83-0e9f9ad58fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1496368 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194/194 [00:02<00:00, 86.50it/s]\n",
      "                   all      0.792      0.954\n",
      "Speed: 0.0ms preprocess, 0.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/val\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915857434272766\n",
      "0.9543688893318176\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model_n = YOLO('./runs/classify/train_yolov8n/weights/last.pt')  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "metrics_n = model_n.val() # no arguments needed, dataset and settings remembered\n",
    "print(metrics_n.top1) # top1 accuracy: 0.7915857434272766\n",
    "print(metrics_n.top5) # top5 accuracy: 0.9543688893318176"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48657b6-d41a-4aab-a734-a157f65613c0",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_n_confusion_matrix_normalized.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df847a0-0732-44dc-aad9-a8d02970ecee",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08101105-de73-4ce4-9f0b-b3cef3a59ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Aquilegia 0.00, Helianthus_Annuus 0.00, Tropaeolum_Majus 0.00, Plumeria 0.00, 2.2ms\n",
      "Speed: 1.2ms preprocess, 2.2ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "results_n = model_n('./assets/snapshots/Viola_Tricolor.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Aquilegia 0.00, Malvaceae 0.00, Helianthus_Annuus 0.00, Plumeria 0.00, 3.6ms\n",
    "# Speed: 0.6ms preprocess, 3.6ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3a64ce-80e5-4357-b6e5-16c2c5191f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 0.99, Anthurium_Andraeanum 0.00, Plumeria 0.00, Alpinia_Purpurata 0.00, Nerine 0.00, 9.7ms\n",
      "Speed: 0.9ms preprocess, 9.7ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_n = model_n('./assets/snapshots/Strelitzia.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 1.00, Alstroemeria 0.00, Guzmania 0.00, Rose 0.00, Anthurium_Andraeanum 0.00, 2.5ms\n",
    "# Speed: 7.1ms preprocess, 2.5ms inference, 0.0ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b158b8-9728-4e4f-9cf7-c700ee7e5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 0.78, Dahlia 0.20, Rose 0.01, Alstroemeria 0.01, Antirrhinum 0.00, 3.1ms\n",
      "Speed: 0.5ms preprocess, 3.1ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_n = model_n('./assets/snapshots/Water_Lilly.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 0.78, Dahlia 0.20, Rose 0.01, Alstroemeria 0.01, Antirrhinum 0.00, 3.1ms\n",
    "# Speed: 6.6ms preprocess, 2.1ms inference, 0.0ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0757c445-e7ed-4284-823a-5595ccff953b",
   "metadata": {},
   "source": [
    "### Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f098460c-dcef-491f-9100-9a7777337f4a",
   "metadata": {},
   "source": [
    "| Format | format Argument | Model | Metadata | Arguments |\n",
    "| -- | -- | -- | -- | -- |\n",
    "| PyTorch | - | yolov8n-cls.pt | âœ… | - |\n",
    "| TorchScript | torchscript | yolov8n-cls.torchscript | âœ… | imgsz, optimize |\n",
    "| ONNX | onnx | yolov8n-cls.onnx | âœ… | imgsz, half, dynamic, simplify, opset |\n",
    "| OpenVINO | openvino | yolov8n-cls_openvino_model/ | âœ… | imgsz, half |\n",
    "| TensorRT | engine | yolov8n-cls.engine | âœ… | imgsz, half, dynamic, simplify, workspace |\n",
    "| CoreML | coreml | yolov8n-cls.mlpackage | âœ… | imgsz, half, int8, nms |\n",
    "| TF SavedModel | saved_model | yolov8n-cls_saved_model/ | âœ… | imgsz, keras |\n",
    "| TF GraphDef | pb | yolov8n-cls.pb | âŒ | imgsz |\n",
    "| TF Lite | tflite | yolov8n-cls.tflite | âœ… | imgsz, half, int8 |\n",
    "| TF Edge TPU | edgetpu | yolov8n-cls_edgetpu.tflite | âœ… | imgsz |\n",
    "| TF.js | tfjs | yolov8n-cls_web_model/ | âœ… | imgsz |\n",
    "| PaddlePaddle | paddle | yolov8n-cls_paddle_model/ | âœ… | imgsz |\n",
    "| ncnn | ncnn | yolov8n-cls_ncnn_model/ | âœ… | imgsz, half |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ccaadc-1647-4133-b988-8455030a520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n.export(format='onnx')\n",
    "model_n.export(format='engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb22c0be-1286-4014-a77d-54759bd885df",
   "metadata": {},
   "source": [
    "## YOLOv8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7967ddf8-9ded-46b4-8e44-f05013b61ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-cls.pt to 'yolov8s-cls.pt'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.2M/12.2M [00:01<00:00, 8.48MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8s-cls.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec72575b-1c2e-433a-bbf8-512e7da858fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.170 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8s-cls.pt, data=./data/Flower_Dataset, epochs=20, patience=50, batch=16, imgsz=64, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=48\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    719408  ultralytics.nn.modules.head.Classify         [512, 48]                     \n",
      "YOLOv8s-cls summary: 99 layers, 5142224 parameters, 5142224 gradients\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /opt/app/data/Flower_Dataset/train... 9206 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9206/9206 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       1/20     0.254G     0.7077          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:15<00:00, 38.01it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 54.12it/s] \n",
      "                   all      0.646      0.903\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       2/20     0.252G     0.2628          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:13<00:00, 43.09it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 53.16it/s] \n",
      "                   all      0.729      0.942\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       3/20     0.252G     0.1659          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 45.42it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 51.91it/s]\n",
      "                   all      0.745      0.949\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       4/20     0.252G     0.1225          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.77it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 49.86it/s] \n",
      "                   all       0.77      0.953\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       5/20     0.252G    0.07941          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.88it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 55.20it/s] \n",
      "                   all      0.776      0.949\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       6/20     0.252G    0.04877          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.38it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 54.50it/s] \n",
      "                   all       0.79      0.956\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       7/20     0.252G     0.0295          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.41it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.60it/s] \n",
      "                   all      0.816       0.96\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       8/20      0.25G    0.01958          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 45.51it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 54.08it/s] \n",
      "                   all      0.809      0.956\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       9/20     0.252G    0.01394          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.00it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 51.31it/s] \n",
      "                   all      0.803      0.957\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      10/20     0.252G   0.009126          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.09it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 53.88it/s] \n",
      "                   all      0.811      0.957\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      11/20     0.252G   0.007711          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.67it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.30it/s] \n",
      "                   all      0.815       0.96\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      12/20     0.252G   0.006201          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 44.87it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 53.78it/s] \n",
      "                   all      0.809       0.96\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      13/20     0.252G   0.005877          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:13<00:00, 43.52it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 53.81it/s] \n",
      "                   all      0.821      0.958\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      14/20     0.245G    0.00423          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 45.06it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 53.72it/s] \n",
      "                   all      0.829      0.963\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      15/20     0.245G   0.002492          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 44.73it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 51.46it/s] \n",
      "                   all      0.825      0.961\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      16/20     0.245G   0.002644          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:13<00:00, 44.15it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.59it/s] \n",
      "                   all      0.825      0.965\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      17/20     0.245G   0.001618          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 45.60it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 53.74it/s] \n",
      "                   all      0.827      0.961\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      18/20     0.245G   0.001335          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 45.46it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 55.07it/s] \n",
      "                   all       0.83      0.965\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      19/20     0.245G   0.001218          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:12<00:00, 46.41it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 53.61it/s] \n",
      "                   all      0.831      0.965\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      20/20     0.245G  0.0008442          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:13<00:00, 43.68it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 54.92it/s]\n",
      "                   all      0.832      0.965\n",
      "\n",
      "20 epochs completed in 0.086 hours.\n",
      "Optimizer stripped from runs/classify/train/weights/last.pt, 10.4MB\n",
      "Optimizer stripped from runs/classify/train/weights/best.pt, 10.4MB\n",
      "Results saved to \u001b[1mruns/classify/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='./data/Flower_Dataset', epochs=20, imgsz=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd61a69d-afe8-4dc4-a84c-ed4a0b68d3b6",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917cf805-b360-4082-9eb7-b15ce62f60be",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_s_curves.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49374e10-71ea-414f-969c-fa3310c591a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "YOLOv8s-cls summary (fused): 73 layers, 5136688 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194/194 [00:02<00:00, 91.72it/s] \n",
      "                   all      0.832      0.965\n",
      "Speed: 0.0ms preprocess, 0.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/val\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8323624134063721\n",
      "0.9650484919548035\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model_s = YOLO('./runs/classify/train_yolov8s/weights/last.pt')  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "metrics_s = model_s.val()  # no arguments needed, dataset and settings remembered\n",
    "print(metrics_s.top1)   # top1 accuracy: 0.8323624134063721\n",
    "print(metrics_s.top5)   # top5 accuracy: 0.9650484919548035"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1198f5-7f66-4d09-a7c4-bae05e9ce56e",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_s_confusion_matrix_normalized.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6537df2-a13a-42ad-b99e-873d04e28245",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ed2962c-9170-4559-9e3b-60ba437de5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Primula 0.00, Malvaceae 0.00, Plumeria 0.00, Datura_Metel 0.00, 2.7ms\n",
      "Speed: 0.5ms preprocess, 2.7ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "results_s = model_s('./assets/snapshots/Viola_Tricolor.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Primula 0.00, Malvaceae 0.00, Plumeria 0.00, Datura_Metel 0.00, 2.7ms\n",
    "# Speed: 0.5ms preprocess, 2.7ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3333f4eb-c95a-4901-b02d-af568950f906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 1.00, Helianthus_Annuus 0.00, Nymphaea_Tetragona 0.00, Plumeria 0.00, Crocus 0.00, 7.6ms\n",
      "Speed: 15.1ms preprocess, 7.6ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_s = model_s('./assets/snapshots/Strelitzia.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 1.00, Helianthus_Annuus 0.00, Nymphaea_Tetragona 0.00, Plumeria 0.00, Crocus 0.00, 7.6ms\n",
    "# Speed: 15.1ms preprocess, 7.6ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4263443-6d8b-4aca-a8ee-ec29ef4abd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 0.99, Alstroemeria 0.01, Passiflora 0.00, Billbergia_Pyramidalis 0.00, Protea_Cynaroides 0.00, 3.2ms\n",
      "Speed: 0.4ms preprocess, 3.2ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_s = model_s('./assets/snapshots/Water_Lilly.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 0.99, Alstroemeria 0.01, Passiflora 0.00, Billbergia_Pyramidalis 0.00, Protea_Cynaroides 0.00, 3.2ms\n",
    "# Speed: 0.4ms preprocess, 3.2ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a430b-1c00-467b-a703-8cacc4343fb7",
   "metadata": {},
   "source": [
    "### Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f840f59-b872-4708-8552-3efb7b2d8c33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CPU (Intel Core(TM) i7-7700 3.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/classify/train_yolov8s/weights/last.pt' with input shape (1, 3, 64, 64) BCHW and output shape(s) (1, 48) (9.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.4s, saved as 'runs/classify/train_yolov8s/weights/last.onnx' (19.6 MB)\n",
      "\n",
      "Export complete (1.6s)\n",
      "Results saved to \u001b[1m/opt/app/runs/classify/train_yolov8s/weights\u001b[0m\n",
      "Predict:         yolo predict task=classify model=runs/classify/train_yolov8s/weights/last.onnx imgsz=64  \n",
      "Validate:        yolo val task=classify model=runs/classify/train_yolov8s/weights/last.onnx imgsz=64 data=./data/Flower_Dataset  \n",
      "Visualize:       https://netron.app\n",
      "WARNING âš ï¸ TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/classify/train_yolov8s/weights/last.pt' with input shape (1, 3, 64, 64) BCHW and output shape(s) (1, 48) (9.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.33...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.8s, saved as 'runs/classify/train_yolov8s/weights/last.onnx' (19.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 8.6.1...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 64, 64) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 48) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as runs/classify/train_yolov8s/weights/last.engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/04/2023-04:38:06] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1961, GPU 1390 (MiB)\n",
      "[09/04/2023-04:38:10] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +226, GPU +38, now: CPU 2187, GPU 1428 (MiB)\n",
      "[09/04/2023-04:38:10] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/04/2023-04:38:10] [TRT] [I] Input filename:   runs/classify/train_yolov8s/weights/last.onnx\n",
      "[09/04/2023-04:38:10] [TRT] [I] ONNX IR version:  0.0.8\n",
      "[09/04/2023-04:38:10] [TRT] [I] Opset version:    17\n",
      "[09/04/2023-04:38:10] [TRT] [I] Producer name:    pytorch\n",
      "[09/04/2023-04:38:10] [TRT] [I] Producer version: 2.0.1\n",
      "[09/04/2023-04:38:10] [TRT] [I] Domain:           \n",
      "[09/04/2023-04:38:10] [TRT] [I] Model version:    0\n",
      "[09/04/2023-04:38:10] [TRT] [I] Doc string:       \n",
      "[09/04/2023-04:38:10] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/04/2023-04:38:10] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/04/2023-04:38:10] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[09/04/2023-04:38:10] [TRT] [I] Graph optimization time: 0.00991916 seconds.\n",
      "[09/04/2023-04:38:10] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[09/04/2023-04:38:10] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/04/2023-04:38:36] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[09/04/2023-04:38:36] [TRT] [I] Total Host Persistent Memory: 116176\n",
      "[09/04/2023-04:38:36] [TRT] [I] Total Device Persistent Memory: 12288\n",
      "[09/04/2023-04:38:36] [TRT] [I] Total Scratch Memory: 0\n",
      "[09/04/2023-04:38:36] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 6 MiB, GPU 26 MiB\n",
      "[09/04/2023-04:38:36] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 86 steps to complete.\n",
      "[09/04/2023-04:38:36] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 1.43062ms to assign 6 blocks to 86 nodes requiring 360960 bytes.\n",
      "[09/04/2023-04:38:36] [TRT] [I] Total Activation Memory: 360448\n",
      "[09/04/2023-04:38:37] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +26, now: CPU 0, GPU 26 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success âœ… 32.0s, saved as 'runs/classify/train_yolov8s/weights/last.engine' (26.0 MB)\n",
      "\n",
      "Export complete (32.0s)\n",
      "Results saved to \u001b[1m/opt/app/runs/classify/train_yolov8s/weights\u001b[0m\n",
      "Predict:         yolo predict task=classify model=runs/classify/train_yolov8s/weights/last.engine imgsz=64  \n",
      "Validate:        yolo val task=classify model=runs/classify/train_yolov8s/weights/last.engine imgsz=64 data=./data/Flower_Dataset  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/classify/train_yolov8s/weights/last.engine'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s.export(format='onnx')\n",
    "model_s.export(format='engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925577b-8058-4138-9d08-9d3d76d2b15a",
   "metadata": {},
   "source": [
    "## YOLOv8m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07e1f81f-362c-4dda-add7-a2f75976651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-cls.pt to 'yolov8m-cls.pt'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32.7M/32.7M [00:03<00:00, 9.59MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8m-cls.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5003a7c-f080-481f-b860-2dfb489de40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.170 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8m-cls.pt, data=./data/Flower_Dataset, epochs=20, patience=50, batch=16, imgsz=64, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=48\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n",
      "  8                  -1  2   7084032  ultralytics.nn.modules.block.C2f             [768, 768, 2, True]           \n",
      "  9                  -1  1   1047088  ultralytics.nn.modules.head.Classify         [768, 48]                     \n",
      "YOLOv8m-cls summary: 141 layers, 15833824 parameters, 15833824 gradients\n",
      "Transferred 228/230 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /opt/app/data/Flower_Dataset/train... 9206 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9206/9206 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 38 weight(decay=0.0), 39 weight(decay=0.0005), 39 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       1/20     0.623G     0.6689          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:26<00:00, 21.64it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 49.67it/s]\n",
      "                   all      0.707      0.928\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       2/20     0.606G     0.2428          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:23<00:00, 24.59it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 50.29it/s]\n",
      "                   all      0.726      0.951\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       3/20     0.606G     0.1773          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:22<00:00, 25.93it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 50.31it/s]\n",
      "                   all      0.731       0.94\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       4/20     0.606G     0.1485          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:22<00:00, 26.07it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 49.08it/s]\n",
      "                   all      0.781      0.947\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       5/20     0.598G    0.09297          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.36it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.18it/s]\n",
      "                   all      0.795      0.955\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       6/20     0.598G    0.05003          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.18it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 50.49it/s]\n",
      "                   all      0.817      0.961\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       7/20     0.598G    0.03295          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.70it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 50.29it/s]\n",
      "                   all      0.821      0.963\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       8/20     0.598G     0.0238          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:24<00:00, 23.36it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 51.82it/s]\n",
      "                   all       0.82      0.963\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       9/20     0.598G    0.01809          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.57it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 51.47it/s]\n",
      "                   all      0.837      0.966\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      10/20     0.598G    0.01624          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.58it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 48.89it/s]\n",
      "                   all      0.833      0.961\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      11/20      0.61G    0.01121          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.58it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.60it/s]\n",
      "                   all      0.829      0.967\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      12/20     0.598G   0.007516          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.63it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 49.32it/s]\n",
      "                   all      0.837      0.966\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      13/20     0.598G    0.00548          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:24<00:00, 23.30it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 50.80it/s]\n",
      "                   all      0.835      0.971\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      14/20     0.598G   0.004503          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.44it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 50.86it/s]\n",
      "                   all       0.84      0.969\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      15/20     0.598G    0.00361          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.20it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 50.05it/s]\n",
      "                   all       0.85       0.97\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      16/20     0.598G   0.002533          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.74it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 49.38it/s]\n",
      "                   all      0.846       0.97\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      17/20     0.598G   0.002293          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.64it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 49.29it/s]\n",
      "                   all      0.852       0.97\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      18/20      0.61G   0.001585          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.66it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.15it/s]\n",
      "                   all      0.856      0.969\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      19/20     0.598G   0.001086          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.55it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 52.46it/s]\n",
      "                   all      0.857       0.97\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      20/20     0.598G   0.001087          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:21<00:00, 26.55it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:01<00:00, 51.38it/s]\n",
      "                   all      0.858       0.97\n",
      "\n",
      "20 epochs completed in 0.149 hours.\n",
      "Optimizer stripped from runs/classify/train/weights/last.pt, 31.8MB\n",
      "Optimizer stripped from runs/classify/train/weights/best.pt, 31.8MB\n",
      "Results saved to \u001b[1mruns/classify/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='./data/Flower_Dataset', epochs=20, imgsz=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc55c2-2034-4067-968b-3d33ace63699",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3df07e-ea75-43f1-b680-5090ff6d2a10",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_m_curves.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "761f8d09-9072-4fad-b9cf-d3cfe3ea263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "YOLOv8m-cls summary (fused): 103 layers, 15824144 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194/194 [00:02<00:00, 87.90it/s]\n",
      "                   all      0.858      0.969\n",
      "Speed: 0.0ms preprocess, 0.4ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/val\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8579287528991699\n",
      "0.9692556262016296\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model_m = YOLO('./runs/classify/train_yolov8m/weights/last.pt')  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "metrics_m = model_m.val()  # no arguments needed, dataset and settings remembered\n",
    "print(metrics_m.top1)   # top1 accuracy: 0.8579287528991699\n",
    "print(metrics_m.top5)   # top5 accuracy: 0.9692556262016296"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b04467-13f6-4af5-8b5d-db42d30ab122",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_m_confusion_matrix_normalized.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb358413-fd6e-402b-b1cb-fd7e155ce3f9",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3e2138a-9cc8-4b51-b71f-ba3dadd893b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Iris_Pseudacorus 0.00, Primula 0.00, Cattleya 0.00, Helianthus_Annuus 0.00, 4.0ms\n",
      "Speed: 0.4ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "results_m = model_m('./assets/snapshots/Viola_Tricolor.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Iris_Pseudacorus 0.00, Primula 0.00, Cattleya 0.00, Helianthus_Annuus 0.00, 4.0ms\n",
    "# Speed: 0.4ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2144dfb0-66ec-4a60-8df6-0325ceea0146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 1.00, Alpinia_Purpurata 0.00, Zantedeschia_Aethiopica 0.00, Helianthus_Annuus 0.00, Plumeria 0.00, 6.0ms\n",
      "Speed: 0.5ms preprocess, 6.0ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_m = model_m('./assets/snapshots/Strelitzia.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 1.00, Alpinia_Purpurata 0.00, Zantedeschia_Aethiopica 0.00, Helianthus_Annuus 0.00, Plumeria 0.00, 6.0ms\n",
    "# Speed: 0.5ms preprocess, 6.0ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c255cb5-52c0-4d0a-bec6-d4bba03e7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 1.00, Dahlia 0.00, Alstroemeria 0.00, Passiflora 0.00, Guzmania 0.00, 5.0ms\n",
      "Speed: 0.5ms preprocess, 5.0ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_m = model_m('./assets/snapshots/Water_Lilly.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 1.00, Dahlia 0.00, Alstroemeria 0.00, Passiflora 0.00, Guzmania 0.00, 5.0ms\n",
    "# Speed: 0.5ms preprocess, 5.0ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc95937-e995-4091-892f-897e18a148f6",
   "metadata": {},
   "source": [
    "### Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c887e15a-6292-472f-b633-07eb09417dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CPU (Intel Core(TM) i7-7700 3.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/classify/train_yolov8m/weights/last.pt' with input shape (1, 3, 64, 64) BCHW and output shape(s) (1, 48) (30.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.0s, saved as 'runs/classify/train_yolov8m/weights/last.onnx' (60.4 MB)\n",
      "\n",
      "Export complete (2.2s)\n",
      "Results saved to \u001b[1m/opt/app/runs/classify/train_yolov8m/weights\u001b[0m\n",
      "Predict:         yolo predict task=classify model=runs/classify/train_yolov8m/weights/last.onnx imgsz=64  \n",
      "Validate:        yolo val task=classify model=runs/classify/train_yolov8m/weights/last.onnx imgsz=64 data=./data/Flower_Dataset  \n",
      "Visualize:       https://netron.app\n",
      "WARNING âš ï¸ TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/classify/train_yolov8m/weights/last.pt' with input shape (1, 3, 64, 64) BCHW and output shape(s) (1, 48) (30.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.33...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 2.5s, saved as 'runs/classify/train_yolov8m/weights/last.onnx' (60.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 8.6.1...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 64, 64) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 48) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as runs/classify/train_yolov8m/weights/last.engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/04/2023-05:05:10] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2291, GPU 1639 (MiB)\n",
      "[09/04/2023-05:05:15] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +227, GPU +38, now: CPU 2518, GPU 1673 (MiB)\n",
      "[09/04/2023-05:05:15] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/04/2023-05:05:15] [TRT] [I] Input filename:   runs/classify/train_yolov8m/weights/last.onnx\n",
      "[09/04/2023-05:05:15] [TRT] [I] ONNX IR version:  0.0.8\n",
      "[09/04/2023-05:05:15] [TRT] [I] Opset version:    17\n",
      "[09/04/2023-05:05:15] [TRT] [I] Producer name:    pytorch\n",
      "[09/04/2023-05:05:15] [TRT] [I] Producer version: 2.0.1\n",
      "[09/04/2023-05:05:15] [TRT] [I] Domain:           \n",
      "[09/04/2023-05:05:15] [TRT] [I] Model version:    0\n",
      "[09/04/2023-05:05:15] [TRT] [I] Doc string:       \n",
      "[09/04/2023-05:05:15] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/04/2023-05:05:15] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/04/2023-05:05:15] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[09/04/2023-05:05:16] [TRT] [I] Graph optimization time: 0.0138118 seconds.\n",
      "[09/04/2023-05:05:16] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[09/04/2023-05:05:16] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/04/2023-05:05:47] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[09/04/2023-05:05:47] [TRT] [I] Total Host Persistent Memory: 175840\n",
      "[09/04/2023-05:05:47] [TRT] [I] Total Device Persistent Memory: 13312\n",
      "[09/04/2023-05:05:47] [TRT] [I] Total Scratch Memory: 0\n",
      "[09/04/2023-05:05:47] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 63 MiB\n",
      "[09/04/2023-05:05:47] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 129 steps to complete.\n",
      "[09/04/2023-05:05:47] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 4.63332ms to assign 8 blocks to 129 nodes requiring 590336 bytes.\n",
      "[09/04/2023-05:05:47] [TRT] [I] Total Activation Memory: 589824\n",
      "[09/04/2023-05:05:47] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +63, now: CPU 0, GPU 63 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success âœ… 39.4s, saved as 'runs/classify/train_yolov8m/weights/last.engine' (63.8 MB)\n",
      "\n",
      "Export complete (39.4s)\n",
      "Results saved to \u001b[1m/opt/app/runs/classify/train_yolov8m/weights\u001b[0m\n",
      "Predict:         yolo predict task=classify model=runs/classify/train_yolov8m/weights/last.engine imgsz=64  \n",
      "Validate:        yolo val task=classify model=runs/classify/train_yolov8m/weights/last.engine imgsz=64 data=./data/Flower_Dataset  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/classify/train_yolov8m/weights/last.engine'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_m.export(format='onnx')\n",
    "model_m.export(format='engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c5911-7905-4dc4-a7f9-1fbe65dc7c83",
   "metadata": {},
   "source": [
    "## YOLOv8l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22cd7db1-81dd-435c-84c3-e18978031163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-cls.pt to 'yolov8l-cls.pt'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71.7M/71.7M [00:08<00:00, 8.70MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8l-cls.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3985dc91-08b4-42cf-82cb-cd9056547876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.170 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8l-cls.pt, data=./data/Flower_Dataset, epochs=20, patience=50, batch=16, imgsz=64, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=48\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   4720640  ultralytics.nn.modules.conv.Conv             [512, 1024, 3, 2]             \n",
      "  8                  -1  3  17836032  ultralytics.nn.modules.block.C2f             [1024, 1024, 3, True]         \n",
      "  9                  -1  1   1374768  ultralytics.nn.modules.head.Classify         [1024, 48]                    \n",
      "YOLOv8l-cls summary: 183 layers, 36261232 parameters, 36261232 gradients\n",
      "Transferred 300/302 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /opt/app/data/Flower_Dataset/train... 9206 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9206/9206 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 50 weight(decay=0.0), 51 weight(decay=0.0005), 51 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       1/20      1.37G     0.6235          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:49<00:00, 11.59it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.72it/s]\n",
      "                   all      0.723      0.936\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       2/20      1.27G     0.2365          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:42<00:00, 13.53it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.07it/s]\n",
      "                   all       0.72      0.934\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       3/20      1.29G     0.2066          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:40<00:00, 14.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 37.49it/s]\n",
      "                   all      0.738      0.933\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       4/20      1.29G     0.1539          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.56it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 39.73it/s]\n",
      "                   all      0.784      0.959\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       5/20      1.28G     0.0965          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:40<00:00, 14.25it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 35.81it/s]\n",
      "                   all      0.808       0.96\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       6/20      1.28G     0.0562          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.42it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.78it/s]\n",
      "                   all       0.82      0.969\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       7/20      1.29G    0.03461          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.65it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 37.39it/s]\n",
      "                   all       0.79      0.958\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       8/20      1.29G    0.03241          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.60it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 37.98it/s]\n",
      "                   all      0.817      0.957\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       9/20      1.28G    0.02251          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.63it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.53it/s]\n",
      "                   all      0.825      0.962\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      10/20      1.29G    0.01523          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.69it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.23it/s]\n",
      "                   all      0.817       0.96\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      11/20      1.28G    0.01228          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:48<00:00, 11.95it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.17it/s]\n",
      "                   all      0.831       0.96\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      12/20      1.28G   0.007472          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.66it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 37.90it/s]\n",
      "                   all      0.829      0.964\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      13/20      1.28G   0.007174          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:38<00:00, 14.89it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 37.91it/s]\n",
      "                   all      0.842      0.968\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      14/20      1.28G   0.007376          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.63it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.58it/s]\n",
      "                   all      0.834      0.971\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      15/20      1.28G   0.004556          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.63it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.41it/s]\n",
      "                   all      0.836      0.965\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      16/20      1.29G   0.002885          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:44<00:00, 12.94it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.12it/s]\n",
      "                   all      0.846      0.968\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      17/20      1.28G   0.002012          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.48it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 37.76it/s]\n",
      "                   all      0.845      0.972\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      18/20      1.29G   0.000997          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.66it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 38.25it/s]\n",
      "                   all      0.847      0.972\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      19/20      1.29G  0.0009713          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:40<00:00, 14.39it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 37.72it/s]\n",
      "                   all      0.848      0.973\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      20/20      1.29G   0.000712          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:39<00:00, 14.50it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:02<00:00, 37.92it/s]\n",
      "                   all       0.85      0.973\n",
      "\n",
      "20 epochs completed in 0.273 hours.\n",
      "Optimizer stripped from runs/classify/train/weights/last.pt, 72.7MB\n",
      "Optimizer stripped from runs/classify/train/weights/best.pt, 72.7MB\n",
      "Results saved to \u001b[1mruns/classify/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='./data/Flower_Dataset', epochs=20, imgsz=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c7ebc-218c-480a-8b6c-efdbfbceb9a9",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d47ef0-22a3-4455-9f83-c3f27fede7bb",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_l_curves.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "496c6c27-51a0-4460-8739-1f83da9756c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "YOLOv8l-cls summary (fused): 133 layers, 36246064 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194/194 [00:02<00:00, 72.36it/s]\n",
      "                   all       0.85      0.973\n",
      "Speed: 0.0ms preprocess, 0.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/val\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849838137626648\n",
      "0.9731391072273254\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model_l = YOLO('./runs/classify/train_yolov8l/weights/last.pt')  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "metrics_l = model_l.val()  # no arguments needed, dataset and settings remembered\n",
    "print(metrics_l.top1)   # top1 accuracy: 0.849838137626648\n",
    "print(metrics_l.top5)   # top5 accuracy: 0.9731391072273254"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6214016-ffcf-492c-a117-b4011b39e51b",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_l_confusion_matrix_normalized.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee9401-bdae-4988-9e82-a24b91094ab8",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d49e4590-bde6-4c40-bf2d-4a0113774a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Malvaceae 0.00, Iris_Pseudacorus 0.00, Primula 0.00, Paphiopedilum 0.00, 6.3ms\n",
      "Speed: 0.5ms preprocess, 6.3ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "results_l = model_l('./assets/snapshots/Viola_Tricolor.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Malvaceae 0.00, Iris_Pseudacorus 0.00, Primula 0.00, Paphiopedilum 0.00, 6.3ms\n",
    "# Speed: 0.5ms preprocess, 6.3ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e250317-0b3f-44d6-893b-e33d576c1e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 1.00, Anthurium_Andraeanum 0.00, Tropaeolum_Majus 0.00, Alstroemeria 0.00, Zantedeschia_Aethiopica 0.00, 8.0ms\n",
      "Speed: 0.6ms preprocess, 8.0ms inference, 0.0ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_l = model_l('./assets/snapshots/Strelitzia.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 1.00, Anthurium_Andraeanum 0.00, Tropaeolum_Majus 0.00, Alstroemeria 0.00, Zantedeschia_Aethiopica 0.00, 8.0ms\n",
    "# Speed: 0.6ms preprocess, 8.0ms inference, 0.0ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eff1cc9e-337d-4a42-8030-a5e9f4b47a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 1.00, Rose 0.00, Primula 0.00, Dahlia 0.00, Alstroemeria 0.00, 7.6ms\n",
      "Speed: 0.4ms preprocess, 7.6ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_l = model_l('./assets/snapshots/Water_Lilly.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 1.00, Rose 0.00, Primula 0.00, Dahlia 0.00, Alstroemeria 0.00, 7.6ms\n",
    "# Speed: 0.4ms preprocess, 7.6ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85c14a2-d42f-47cc-9db5-214478835f44",
   "metadata": {},
   "source": [
    "### Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa65c4bc-10d8-42fb-aa03-72499d537076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CPU (Intel Core(TM) i7-7700 3.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/classify/train_yolov8l/weights/last.pt' with input shape (1, 3, 64, 64) BCHW and output shape(s) (1, 48) (69.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 2.7s, saved as 'runs/classify/train_yolov8l/weights/last.onnx' (138.3 MB)\n",
      "\n",
      "Export complete (4.0s)\n",
      "Results saved to \u001b[1m/opt/app/runs/classify/train_yolov8l/weights\u001b[0m\n",
      "Predict:         yolo predict task=classify model=runs/classify/train_yolov8l/weights/last.onnx imgsz=64  \n",
      "Validate:        yolo val task=classify model=runs/classify/train_yolov8l/weights/last.onnx imgsz=64 data=./data/Flower_Dataset  \n",
      "Visualize:       https://netron.app\n",
      "WARNING âš ï¸ TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/classify/train_yolov8l/weights/last.pt' with input shape (1, 3, 64, 64) BCHW and output shape(s) (1, 48) (69.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.33...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 6.5s, saved as 'runs/classify/train_yolov8l/weights/last.onnx' (138.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 8.6.1...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 64, 64) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 48) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as runs/classify/train_yolov8l/weights/last.engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/04/2023-05:37:50] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2549, GPU 2223 (MiB)\n",
      "[09/04/2023-05:37:54] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +226, GPU +38, now: CPU 2775, GPU 2265 (MiB)\n",
      "[09/04/2023-05:37:54] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/04/2023-05:37:54] [TRT] [I] Input filename:   runs/classify/train_yolov8l/weights/last.onnx\n",
      "[09/04/2023-05:37:54] [TRT] [I] ONNX IR version:  0.0.8\n",
      "[09/04/2023-05:37:54] [TRT] [I] Opset version:    17\n",
      "[09/04/2023-05:37:54] [TRT] [I] Producer name:    pytorch\n",
      "[09/04/2023-05:37:54] [TRT] [I] Producer version: 2.0.1\n",
      "[09/04/2023-05:37:54] [TRT] [I] Domain:           \n",
      "[09/04/2023-05:37:54] [TRT] [I] Model version:    0\n",
      "[09/04/2023-05:37:54] [TRT] [I] Doc string:       \n",
      "[09/04/2023-05:37:54] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/04/2023-05:37:55] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/04/2023-05:37:55] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[09/04/2023-05:37:55] [TRT] [I] Graph optimization time: 0.0178621 seconds.\n",
      "[09/04/2023-05:37:55] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[09/04/2023-05:37:55] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/04/2023-05:38:31] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[09/04/2023-05:38:31] [TRT] [I] Total Host Persistent Memory: 192416\n",
      "[09/04/2023-05:38:31] [TRT] [I] Total Device Persistent Memory: 15360\n",
      "[09/04/2023-05:38:31] [TRT] [I] Total Scratch Memory: 512\n",
      "[09/04/2023-05:38:31] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 18 MiB, GPU 166 MiB\n",
      "[09/04/2023-05:38:31] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 174 steps to complete.\n",
      "[09/04/2023-05:38:31] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 6.70158ms to assign 13 blocks to 174 nodes requiring 1017856 bytes.\n",
      "[09/04/2023-05:38:31] [TRT] [I] Total Activation Memory: 1015808\n",
      "[09/04/2023-05:38:31] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +166, now: CPU 0, GPU 166 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success âœ… 49.3s, saved as 'runs/classify/train_yolov8l/weights/last.engine' (166.3 MB)\n",
      "\n",
      "Export complete (49.3s)\n",
      "Results saved to \u001b[1m/opt/app/runs/classify/train_yolov8l/weights\u001b[0m\n",
      "Predict:         yolo predict task=classify model=runs/classify/train_yolov8l/weights/last.engine imgsz=64  \n",
      "Validate:        yolo val task=classify model=runs/classify/train_yolov8l/weights/last.engine imgsz=64 data=./data/Flower_Dataset  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/classify/train_yolov8l/weights/last.engine'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l.export(format='onnx')\n",
    "model_l.export(format='engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c89dcf-7b6b-4d96-8782-04a8a2e644c3",
   "metadata": {},
   "source": [
    "## YOLOv8x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9fcdb8a-fb61-4bb5-ab2f-e4660c8e08f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-cls.pt to 'yolov8x-cls.pt'...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110M/110M [00:33<00:00, 3.47MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8x-cls.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b630586-83e9-4e2f-a146-654329f5d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.170 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8x-cls.pt, data=./data/Flower_Dataset, epochs=20, patience=50, batch=16, imgsz=64, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=48\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   7375360  ultralytics.nn.modules.conv.Conv             [640, 1280, 3, 2]             \n",
      "  8                  -1  3  27865600  ultralytics.nn.modules.block.C2f             [1280, 1280, 3, True]         \n",
      "  9                  -1  1   1702448  ultralytics.nn.modules.head.Classify         [1280, 48]                    \n",
      "YOLOv8x-cls summary: 183 layers, 56203328 parameters, 56203328 gradients\n",
      "Transferred 300/302 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /opt/app/data/Flower_Dataset/train... 9206 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9206/9206 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 50 weight(decay=0.0), 51 weight(decay=0.0005), 51 bias(decay=0.0)\n",
      "Image sizes 64 train, 64 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       1/20       2.2G     0.5923          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [01:11<00:00,  8.04it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.81it/s]\n",
      "                   all      0.721      0.931\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       2/20      2.06G     0.2462          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [01:16<00:00,  7.55it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.18it/s]\n",
      "                   all       0.72      0.937\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       3/20      2.06G      0.218          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [01:21<00:00,  7.05it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.61it/s]\n",
      "                   all       0.73      0.926\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       4/20      2.06G     0.1603          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:56<00:00, 10.16it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.84it/s]\n",
      "                   all      0.781      0.946\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       5/20      2.06G     0.1049          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:56<00:00, 10.19it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.51it/s]\n",
      "                   all      0.781      0.948\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       6/20      2.06G    0.07317          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:56<00:00, 10.12it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.23it/s]\n",
      "                   all      0.808       0.96\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       7/20      2.06G     0.0434          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:57<00:00, 10.08it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.32it/s]\n",
      "                   all      0.823      0.964\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       8/20      2.06G    0.02899          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:57<00:00, 10.06it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.00it/s]\n",
      "                   all      0.817      0.959\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       9/20      2.06G    0.02277          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:56<00:00, 10.23it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.88it/s]\n",
      "                   all      0.815      0.961\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      10/20      2.06G    0.01818          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:55<00:00, 10.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.85it/s]\n",
      "                   all      0.828      0.963\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      11/20      2.06G     0.0139          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [01:01<00:00,  9.36it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.70it/s]\n",
      "                   all      0.836      0.968\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      12/20      2.06G    0.01214          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:55<00:00, 10.31it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 22.63it/s]\n",
      "                   all      0.838      0.962\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      13/20      2.06G   0.008583          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [01:03<00:00,  9.05it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.19it/s]\n",
      "                   all      0.835      0.964\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      14/20      2.06G   0.004023          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [01:00<00:00,  9.51it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.47it/s]\n",
      "                   all       0.85      0.967\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      15/20      2.06G   0.003141          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:55<00:00, 10.37it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.75it/s]\n",
      "                   all      0.851      0.969\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      16/20      2.06G   0.002873          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:55<00:00, 10.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.91it/s]\n",
      "                   all      0.848      0.969\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      17/20      2.06G   0.002523          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:55<00:00, 10.36it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.53it/s]\n",
      "                   all      0.853      0.969\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      18/20      2.06G    0.00166          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:55<00:00, 10.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.73it/s]\n",
      "                   all      0.853      0.968\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      19/20      2.06G   0.001289          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:55<00:00, 10.34it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.84it/s]\n",
      "                   all      0.858      0.969\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "      20/20      2.06G   0.001081          6         64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 576/576 [00:55<00:00, 10.32it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97/97 [00:04<00:00, 23.47it/s]\n",
      "                   all      0.861      0.971\n",
      "\n",
      "20 epochs completed in 0.416 hours.\n",
      "Optimizer stripped from runs/classify/train/weights/last.pt, 112.6MB\n",
      "Optimizer stripped from runs/classify/train/weights/best.pt, 112.6MB\n",
      "Results saved to \u001b[1mruns/classify/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='./data/Flower_Dataset', epochs=20, imgsz=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d9db5-592d-47ae-b484-68fbfdba3e2a",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4377e2fe-a654-4c17-8263-22677b83d30e",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_x_curves.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6efe19f-07e7-4768-91c4-7c76f034a5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "YOLOv8x-cls summary (fused): 133 layers, 56184688 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /opt/app/data/Flower_Dataset/train... found 9206 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /opt/app/data/Flower_Dataset/val... found 3090 images in 48 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /opt/app/data/Flower_Dataset/val... 3090 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3090/3090 [00:00<?, ?it/s]\u001b[0m\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194/194 [00:03<00:00, 53.22it/s]\n",
      "                   all      0.861      0.971\n",
      "Speed: 0.0ms preprocess, 1.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/val\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8605177402496338\n",
      "0.9708737730979919\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model_x = YOLO('./runs/classify/train_yolov8x/weights/last.pt')  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "metrics_x = model_x.val()  # no arguments needed, dataset and settings remembered\n",
    "print(metrics_x.top1)   # top1 accuracy: 0.8605177402496338\n",
    "print(metrics_x.top5)   # top5 accuracy: 0.9708737730979919"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1982478-86e0-415d-b35c-6b809da3a5fa",
   "metadata": {},
   "source": [
    "![YOLOv8 Image Classifier](./assets/model_x_confusion_matrix_normalized.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5265028-833c-49dc-8570-be6b7abeb9d4",
   "metadata": {},
   "source": [
    "### Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adfff251-a33d-4fa8-8ce7-34b266a7fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Dahlia 0.00, Tropaeolum_Majus 0.00, Rose 0.00, Primula 0.00, 10.1ms\n",
      "Speed: 0.5ms preprocess, 10.1ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "results_x = model_x('./assets/snapshots/Viola_Tricolor.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Viola_Tricolor.jpg: 64x64 Viola 1.00, Dahlia 0.00, Tropaeolum_Majus 0.00, Rose 0.00, Primula 0.00, 10.1ms\n",
    "# Speed: 0.5ms preprocess, 10.1ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdecdba5-b6ab-438f-92a0-40a9d225b76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 1.00, Tropaeolum_Majus 0.00, Alstroemeria 0.00, Rose 0.00, Helianthus_Annuus 0.00, 24.0ms\n",
      "Speed: 5.5ms preprocess, 24.0ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_x = model_x('./assets/snapshots/Strelitzia.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Strelitzia.jpg: 64x64 Strelitzia_Reginae 1.00, Tropaeolum_Majus 0.00, Alstroemeria 0.00, Rose 0.00, Helianthus_Annuus 0.00, 24.0ms\n",
    "# Speed: 5.5ms preprocess, 24.0ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d20ccd3f-9b62-4359-aec5-f2623f4a1012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 0.99, Rose 0.00, Alstroemeria 0.00, Primula 0.00, Protea_Cynaroides 0.00, 9.1ms\n",
      "Speed: 0.5ms preprocess, 9.1ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "results_x = model_x('./assets/snapshots/Water_Lilly.jpg')\n",
    "\n",
    "# image 1/1 /opt/app/assets/snapshots/Water_Lilly.jpg: 64x64 Nymphaea_Tetragona 0.99, Rose 0.00, Alstroemeria 0.00, Primula 0.00, Protea_Cynaroides 0.00, 9.1ms\n",
    "# Speed: 0.5ms preprocess, 9.1ms inference, 0.1ms postprocess per image at shape (1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c8d6b-ddd0-415a-87e4-5941949446e1",
   "metadata": {},
   "source": [
    "### Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7f77b07-0d5a-4774-8b60-7851a998a69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CPU (Intel Core(TM) i7-7700 3.60GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/classify/train_yolov8x/weights/last.pt' with input shape (1, 3, 64, 64) BCHW and output shape(s) (1, 48) (107.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 4.6s, saved as 'runs/classify/train_yolov8x/weights/last.onnx' (214.4 MB)\n",
      "\n",
      "Export complete (5.9s)\n",
      "Results saved to \u001b[1m/opt/app/runs/classify/train_yolov8x/weights\u001b[0m\n",
      "Predict:         yolo predict task=classify model=runs/classify/train_yolov8x/weights/last.onnx imgsz=64  \n",
      "Validate:        yolo val task=classify model=runs/classify/train_yolov8x/weights/last.onnx imgsz=64 data=./data/Flower_Dataset  \n",
      "Visualize:       https://netron.app\n",
      "WARNING âš ï¸ TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics YOLOv8.0.169 ðŸš€ Python-3.10.11 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1060 6GB, 6070MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/classify/train_yolov8x/weights/last.pt' with input shape (1, 3, 64, 64) BCHW and output shape(s) (1, 48) (107.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.33...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 11.5s, saved as 'runs/classify/train_yolov8x/weights/last.onnx' (214.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 8.6.1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/04/2023-06:13:57] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2940, GPU 3092 (MiB)\n",
      "[09/04/2023-06:14:02] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +226, GPU +38, now: CPU 3166, GPU 3118 (MiB)\n",
      "[09/04/2023-06:14:02] [TRT] [I] ----------------------------------------------------------------\n",
      "[09/04/2023-06:14:02] [TRT] [I] Input filename:   runs/classify/train_yolov8x/weights/last.onnx\n",
      "[09/04/2023-06:14:02] [TRT] [I] ONNX IR version:  0.0.8\n",
      "[09/04/2023-06:14:02] [TRT] [I] Opset version:    17\n",
      "[09/04/2023-06:14:02] [TRT] [I] Producer name:    pytorch\n",
      "[09/04/2023-06:14:02] [TRT] [I] Producer version: 2.0.1\n",
      "[09/04/2023-06:14:02] [TRT] [I] Domain:           \n",
      "[09/04/2023-06:14:02] [TRT] [I] Model version:    0\n",
      "[09/04/2023-06:14:02] [TRT] [I] Doc string:       \n",
      "[09/04/2023-06:14:02] [TRT] [I] ----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 64, 64) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 48) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP32 engine as runs/classify/train_yolov8x/weights/last.engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/04/2023-06:14:02] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/04/2023-06:14:02] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[09/04/2023-06:14:02] [TRT] [I] Graph optimization time: 0.0183985 seconds.\n",
      "[09/04/2023-06:14:02] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[09/04/2023-06:14:02] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/04/2023-06:14:41] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[09/04/2023-06:14:41] [TRT] [I] Total Host Persistent Memory: 218000\n",
      "[09/04/2023-06:14:41] [TRT] [I] Total Device Persistent Memory: 14336\n",
      "[09/04/2023-06:14:41] [TRT] [I] Total Scratch Memory: 0\n",
      "[09/04/2023-06:14:41] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 28 MiB, GPU 225 MiB\n",
      "[09/04/2023-06:14:41] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 170 steps to complete.\n",
      "[09/04/2023-06:14:41] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 8.46388ms to assign 12 blocks to 170 nodes requiring 1148416 bytes.\n",
      "[09/04/2023-06:14:41] [TRT] [I] Total Activation Memory: 1146880\n",
      "[09/04/2023-06:14:41] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +225, now: CPU 0, GPU 225 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success âœ… 57.4s, saved as 'runs/classify/train_yolov8x/weights/last.engine' (225.1 MB)\n",
      "\n",
      "Export complete (57.4s)\n",
      "Results saved to \u001b[1m/opt/app/runs/classify/train_yolov8x/weights\u001b[0m\n",
      "Predict:         yolo predict task=classify model=runs/classify/train_yolov8x/weights/last.engine imgsz=64  \n",
      "Validate:        yolo val task=classify model=runs/classify/train_yolov8x/weights/last.engine imgsz=64 data=./data/Flower_Dataset  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/classify/train_yolov8x/weights/last.engine'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_x.export(format='onnx')\n",
    "model_x.export(format='engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac2937-8d69-4357-bad0-06c8186046db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
